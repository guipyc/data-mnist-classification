{"cells":[{"cell_type":"markdown","metadata":{"id":"nz1MkbOrsjms"},"source":["# MNIST Classification"]},{"cell_type":"markdown","metadata":{"id":"t9_CAOTosjmv"},"source":["üéØ <b><u>Exercise objectives</u></b>\n","- Understand the *MNIST* dataset \n","- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n","    - what are *Convolutional Layers*? \n","    - how many *parameters* are involved in such a layer?\n","- Train this CNN on images"]},{"cell_type":"markdown","metadata":{"id":"6RVRiVgmsjmw"},"source":["üöÄ <b><u>Let's get started!</u></b>\n","\n","Imagine that we are  back in time into the 90's.\n","You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n","\n","This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n","\n","From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n","\n","> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"]},{"cell_type":"markdown","metadata":{"id":"M8Bj3yOzsjmx"},"source":["![Number recognition](recognition.gif)\n","\n","*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"]},{"cell_type":"markdown","metadata":{"id":"vtgVYStksjmx"},"source":["ü§î <b><u>How does this CNN work ?</u></b>\n","\n","- *Inputs*: Images (_each image shows a handwritten digit_)\n","- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n","    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n","\n","üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."]},{"cell_type":"code","execution_count":93,"metadata":{"id":"vVvfUmwNsjmy","executionInfo":{"status":"ok","timestamp":1668007935740,"user_tz":180,"elapsed":317,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"ZMU_QR6asjmy"},"source":["## (1) The `MNIST` Dataset"]},{"cell_type":"markdown","metadata":{"id":"-iFvGigRsjmz"},"source":["üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n","- *Vectors*: `boston_housing` (regression)\n","- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n","- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n","\n","\n","üíæ You can **load the MNIST dataset** with the following commands:"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CK0Bh6pIsjm0","executionInfo":{"status":"ok","timestamp":1668007937528,"user_tz":180,"elapsed":479,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"357a44bf-a105-4c45-f7a0-942b94e94eb3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"]},"metadata":{},"execution_count":94}],"source":["from tensorflow.keras import datasets\n","\n","\n","# Loading the MNIST Dataset...\n","(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n","\n","# The train set contains 60 000 images, each of them of size 28x28\n","# The test set contains 10 000 images, each of them of size 28x28\n","(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"N5_lJbhwsjm0"},"source":["### (1.1) Exploring the dataset"]},{"cell_type":"markdown","metadata":{"id":"Y2XcQErHsjm0"},"source":["‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n","\n","üñ® Print some images from the *train set*.\n","\n","<details>\n","    <summary><i>Hints</i></summary>\n","\n","üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n","\n","ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n","    \n","</details>"]},{"cell_type":"code","execution_count":95,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"FpEkV_kJsjm1","executionInfo":{"status":"ok","timestamp":1668007939138,"user_tz":180,"elapsed":293,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"71b9f5e7-859b-43e7-e7b4-6c8d3bb21b7e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["plt.imshow(X_train[10], cmap='gray');\n"]},{"cell_type":"code","source":["plt.imshow(X_train[0], cmap='gray');\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"XddW-vx9yl26","executionInfo":{"status":"ok","timestamp":1668006464449,"user_tz":180,"elapsed":379,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"1396db91-3596-420b-c5ac-13705af466b8"},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["plt.imshow(X_train[1], cmap='gray');\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"C6GULcpHynNx","executionInfo":{"status":"ok","timestamp":1668006465345,"user_tz":180,"elapsed":3,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"5fdd9f71-9edb-41cd-f07d-680bb3d00bfc"},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"pKWoRRdmsjm1"},"source":["### (1.2) Image Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"dWoWKuA3sjm1"},"source":["‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n","\n","üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n","* The `RBG` intensities are coded between 0 and 255. \n","* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"]},{"cell_type":"markdown","metadata":{"id":"4TgMQkA_sjm1"},"source":["‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n","\n","Don't forget to do it both on your train data and your test data.\n","\n","(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"]},{"cell_type":"code","execution_count":96,"metadata":{"tags":["challengify"],"id":"AdvGZHR_sjm1","executionInfo":{"status":"ok","timestamp":1668007944458,"user_tz":180,"elapsed":2807,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["from tensorflow.keras.layers import Normalization \n","normalizer = Normalization() \n","normalizer.adapt(X_train)"]},{"cell_type":"markdown","metadata":{"id":"VuLsNYL6sjm2"},"source":["### (1.3) Inputs' dimensionality"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29IBzTMPsjm2","executionInfo":{"status":"ok","timestamp":1668007945417,"user_tz":180,"elapsed":6,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"14ca580c-8a92-49e5-aac7-56e3160ba80e"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n","(10000, 28, 28)\n"]}],"source":["print(X_train.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Ifx6-voRsjm2"},"source":["üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n","\n","> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n","\n","> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n","\n","üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n","<br>\n","<details>\n","    <summary><i>Answer<i></summary>\n","        \n","* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n","        \n","    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n","        \n","    * In comparison, colored pictures need multiple channels:\n","        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n","        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n","        \n","        \n","</details>        "]},{"cell_type":"markdown","metadata":{"id":"kp92U5IBsjm2"},"source":["‚ùì **Question: expanding dimensions** ‚ùì\n","\n","* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n","\n","* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."]},{"cell_type":"code","execution_count":98,"metadata":{"id":"JZjLmSSnsjm2","executionInfo":{"status":"ok","timestamp":1668007950137,"user_tz":180,"elapsed":235,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["from tensorflow.keras.backend import expand_dims\n","X_train = expand_dims(X_train)\n","X_test = expand_dims(X_test)"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQ6GFeZZsjm2","executionInfo":{"status":"ok","timestamp":1668007951809,"user_tz":180,"elapsed":366,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"8f2d7223-b769-43df-e185-540b73236a80"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28, 1)\n","(10000, 28, 28, 1)\n"]}],"source":["print(X_train_dims.shape)\n","print(X_test_dims.shape)"]},{"cell_type":"markdown","metadata":{"id":"PJnXZL4Esjm2"},"source":["### (1.4) Target encoding"]},{"cell_type":"markdown","metadata":{"id":"ZSO3kGB2sjm3"},"source":["One more thing to for a multiclass classification task in Deep Leaning:\n","\n","üëâ _\"one-hot-encode\" the categories*_\n","\n","‚ùì **Question: encoding the labels** ‚ùì \n","\n","* Use **`to_categorical`** to transform your labels. \n","* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."]},{"cell_type":"code","execution_count":100,"metadata":{"id":"44n90Lhrsjm3","executionInfo":{"status":"ok","timestamp":1668007955407,"user_tz":180,"elapsed":2,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","\n","y_train_cat = to_categorical(y_train)\n","y_test_cat = to_categorical(y_test)\n"]},{"cell_type":"code","execution_count":101,"metadata":{"id":"Rbc8KCv1sjm3","executionInfo":{"status":"ok","timestamp":1668007956844,"user_tz":180,"elapsed":2,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["# Quick check that you correctly used to_categorical\n","assert(y_train_cat.shape == (60000,10))\n","assert(y_test_cat.shape == (10000,10))"]},{"cell_type":"markdown","metadata":{"id":"ZUb2C5OCsjm3"},"source":["The data is now ready to be used. ‚úÖ"]},{"cell_type":"markdown","metadata":{"id":"iivQ82i0sjm3"},"source":["## (2) The Convolutional Neural Network"]},{"cell_type":"markdown","metadata":{"id":"Fsk35iNCsjm3"},"source":["### (2.1) Architecture and compilation of a CNN"]},{"cell_type":"markdown","metadata":{"id":"F7mGU2bosjm3"},"source":["\n","‚ùì **Question: CNN Architecture and compilation** ‚ùì\n","\n","Now, let's build a <u>Convolutional Neural Network</u> that has: \n","\n","\n","- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n","- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n","- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n","- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n","\n","\n","- a `Flatten` layer\n","- a first `Dense` layer with 10 neurons and the `relu` activation function\n","- a last (predictive) layer that is suited for your task\n","\n","In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n","* optimizes the `categorical_crossentropy` loss function,\n","* with the `adam` optimizer, \n","* and the `accuracy` as the metrics\n","\n","(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"]},{"cell_type":"code","execution_count":109,"metadata":{"id":"rGgq-TDJsjm3","executionInfo":{"status":"ok","timestamp":1668008281198,"user_tz":180,"elapsed":457,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["from tensorflow.keras import layers\n","from tensorflow.keras import models\n","from tensorflow.keras import optimizers\n","\n","\n","def initialize_model():\n","\n","    model = models.Sequential()\n","\n","    ### First Convolution & MaxPooling\n","    model.add(layers.Conv2D(8, kernel_size=(4, 4), input_shape=(X_train.shape[1:4]), padding='same', activation=\"relu\"))\n","    model.add(layers.MaxPool2D(pool_size=(2,2))) \n","\n","    ### Second Convolution & MaxPooling\n","    model.add(layers.Conv2D(16, kernel_size=(3, 3), activation=\"relu\")) \n","    model.add(layers.MaxPool2D(pool_size=(2,2))) \n","\n","    ### Flattening\n","    model.add(layers.Flatten())\n","\n","    \n","    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n","    model.add(layers.Dense(10, activation='relu'))\n","\n","    \n","    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n","    model.add(layers.Dense(10, activation='softmax'))\n","\n","\n","    \n","    ### Model compilation\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"code","source":["X_train.shape[1:4]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vOFunQzOChLI","executionInfo":{"status":"ok","timestamp":1668008254097,"user_tz":180,"elapsed":1,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"296318dc-0f9b-41db-9f62-c201ca177903"},"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([28, 28, 1])"]},"metadata":{},"execution_count":108}]},{"cell_type":"markdown","metadata":{"id":"0W2U4Dxvsjm3"},"source":["‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n","\n","How many trainable parameters are there in your model?\n","1. Compute them with ***model.summary( )*** first\n","2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."]},{"cell_type":"code","execution_count":110,"metadata":{"tags":["challengify"],"id":"xV1nxT8Rsjm4","executionInfo":{"status":"ok","timestamp":1668008286532,"user_tz":180,"elapsed":3,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}}},"outputs":[],"source":["model = initialize_model()\n"]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAjNqWhj-fC2","executionInfo":{"status":"ok","timestamp":1668008288643,"user_tz":180,"elapsed":289,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"2620ed12-a370-4c7a-a59f-96c1e0d454b0"},"execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_13\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_20 (Conv2D)          (None, 28, 28, 8)         136       \n","                                                                 \n"," max_pooling2d_16 (MaxPoolin  (None, 14, 14, 8)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_21 (Conv2D)          (None, 12, 12, 16)        1168      \n","                                                                 \n"," max_pooling2d_17 (MaxPoolin  (None, 6, 6, 16)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_7 (Flatten)         (None, 576)               0         \n","                                                                 \n"," dense_14 (Dense)            (None, 10)                5770      \n","                                                                 \n"," dense_15 (Dense)            (None, 10)                110       \n","                                                                 \n","=================================================================\n","Total params: 7,184\n","Trainable params: 7,184\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"UuHugiGJsjm4"},"source":["### (2.2) Training a CNN"]},{"cell_type":"markdown","metadata":{"id":"NFV1Qh9esjm4"},"source":["‚ùì **Question: training a CNN** ‚ùì \n","\n","Initialize your model and fit it on the train data. \n","- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n","- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"]},{"cell_type":"code","execution_count":113,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUnjpnx3sjm4","executionInfo":{"status":"ok","timestamp":1668008344692,"user_tz":180,"elapsed":22273,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"4ebce40f-353c-404d-b992-48ccaae93059"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","2625/2625 [==============================] - 11s 4ms/step - loss: 0.1816 - accuracy: 0.7392 - val_loss: 0.0474 - val_accuracy: 0.9366\n","Epoch 2/5\n","2625/2625 [==============================] - 11s 4ms/step - loss: 0.0396 - accuracy: 0.9512 - val_loss: 0.0334 - val_accuracy: 0.9582\n"]}],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","es = EarlyStopping()\n","\n","model = initialize_model()\n","\n","history = model.fit(X_train, y_train_cat,\n","                    validation_split = 0.3,\n","                    epochs = 5,\n","                    batch_size = 16, \n","                    verbose = 1, \n","                    callbacks = [es])\n"]},{"cell_type":"markdown","metadata":{"id":"AuaT15rFsjm4"},"source":["‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n","\n","_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"]},{"cell_type":"markdown","metadata":{"tags":["challengify"],"id":"eC55GS1Lsjm4"},"source":["> YOUR ANSWER HERE"]},{"cell_type":"markdown","metadata":{"id":"VxpaWIXksjm4"},"source":["<details>\n","    <summary><i>Answer</i></summary>\n","\n","With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n","    \n","Remember that we've just trained our CNN model on $60000$ training images\n","\n","If the chosen batch size is 32: \n","\n","* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n","* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n","    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n","    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n","    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n","\n","\n","üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n","\n","</details>    \n"]},{"cell_type":"markdown","metadata":{"id":"t-SnT0VRsjm4"},"source":["### (2.3) Evaluating its performance"]},{"cell_type":"markdown","metadata":{"id":"muhj_Pz0sjm4"},"source":["‚ùì **Question: Evaluating your CNN** ‚ùì \n","\n","What is your **`accuracy on the test set?`**"]},{"cell_type":"code","execution_count":115,"metadata":{"tags":["challengify"],"colab":{"base_uri":"https://localhost:8080/"},"id":"Cvxs9hTqsjm4","executionInfo":{"status":"ok","timestamp":1668008363291,"user_tz":180,"elapsed":1022,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"6c2577cf-ab90-4f85-9f0d-ed2e3d7cc1c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 3ms/step - loss: 0.0313 - accuracy: 0.9601\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.0313187874853611, 0.960099995136261]"]},"metadata":{},"execution_count":115}],"source":["model.evaluate(X_test,y_test_cat)"]},{"cell_type":"code","source":["model.evaluate(X_train,y_train_cat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mayr_C_iBIQD","executionInfo":{"status":"ok","timestamp":1668008378313,"user_tz":180,"elapsed":5273,"user":{"displayName":"Guilherme Cavalcanti","userId":"07832140799484720354"}},"outputId":"dc74b84b-3b49-4bd8-b388-25b7effc2fe0"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["1875/1875 [==============================] - 5s 3ms/step - loss: 0.0313 - accuracy: 0.9613\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.0313408188521862, 0.9612833261489868]"]},"metadata":{},"execution_count":116}]},{"cell_type":"markdown","metadata":{"id":"H1ENnPf-sjm4"},"source":["üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n","\n","üî• You solved what was a very hard problem 30 years ago with your own CNN."]},{"cell_type":"markdown","metadata":{"id":"AZCQy8wksjm5"},"source":["üèÅ **Congratulations!**\n","\n","üíæ Don't forget to `git add/commit/push` your notebook...\n","\n","üöÄ ... and move on to the next challenge!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[{"file_id":"1HuoLg3OIeQ7paPZVUrGXf6q3sRywMDEm","timestamp":1668008477852}]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}